{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1404_Assignment03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-cuWtt61o1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b6dc38-95c6-4d62-dc23-a388a022dafd"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9PefRsWu2aC"
      },
      "source": [
        "import string\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfl5BgyHp8FZ",
        "outputId": "23e044a8-0f79-4b8a-deae-8708a8788a9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiFlC-hJrpHT",
        "outputId": "ab3d0ecc-184d-473a-a464-24ba2f4a02a7"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6o1LbdOrkk5"
      },
      "source": [
        "def read_files(file_loc):\n",
        "  '''\n",
        "  This function reads txt data from a file in the drive\n",
        "\n",
        "  args - a string containing the files location\n",
        "  returns - a list containing the text data\n",
        "  '''\n",
        "  dataset = []\n",
        "  with open(file_loc, 'r') as train_file:\n",
        "    # i = 0\n",
        "    # max = 20000\n",
        "    for line in train_file:\n",
        "      # if max>=i:\n",
        "        dataset.append(line)\n",
        "      # else: \n",
        "      #   break\n",
        "      # i += 1\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETu3E27HvWJ1"
      },
      "source": [
        "def separate_labels(dataset):\n",
        "  '''This function will separate the labels/class and examples/documents from the dataset'''\n",
        "  labels = []\n",
        "  documents = []\n",
        "\n",
        "  for line in dataset:\n",
        "    splitted_line = line.strip().split('\\t', 2)\n",
        "    labels.append(splitted_line[1])\n",
        "    documents.append(splitted_line[2])\n",
        "\n",
        "  return labels, documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gauP4rumyvi8"
      },
      "source": [
        "def remove_url(documents):\n",
        "  '''This function removes URL's from Texts'''\n",
        "  url_removed = []\n",
        "\n",
        "  # Your code here\n",
        "  for line in documents:\n",
        "    url_removed.append(re.sub('http[s]?://\\S+', '', line))\n",
        "\n",
        "  return url_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImWgQZob8Lbj"
      },
      "source": [
        "def remove_hashtag(documents):\n",
        "  '''This function will remove all occurences of # from the texts'''\n",
        "  hashtag_removed = []\n",
        "\n",
        "  # map hashtag to space\n",
        "  translator = str.maketrans('#', ' '*len('#'), '')\n",
        "\n",
        "  for line in documents:\n",
        "    hashtag_removed.append(line.translate(translator))\n",
        "\n",
        "  return hashtag_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoTN1D839rXY"
      },
      "source": [
        "def remove_whitespaces(documents):\n",
        "  '''This function removes multiple whitespaces and replace them with a single whitespace'''\n",
        "  whitespace_removed = []\n",
        "\n",
        "  for line in documents:\n",
        "    whitespace_removed.append(' '.join(line.split()))\n",
        "\n",
        "  return whitespace_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cnfwDnMRZ3q"
      },
      "source": [
        "def text_lowercasing(documents):\n",
        "  lowercased_docs = []\n",
        "\n",
        "  for line in documents:\n",
        "    lowercased_docs.append(line.lower())\n",
        "\n",
        "  return lowercased_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWXJjkiVy3TJ"
      },
      "source": [
        "def tokenize_sentence(documents):\n",
        "  '''This function takes a line and provides tokens/words by splitting them using NLTK'''\n",
        "  \n",
        "  tokenized_docs = []\n",
        "  \n",
        "  for line in documents:\n",
        "    tokenized_docs.append(word_tokenize(line))\n",
        "\n",
        "  return tokenized_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05_cbMJOocUE"
      },
      "source": [
        "def char_n_gram_ready(documents):\n",
        "  '''An n-gram is a contiguous sequence of n items from a given sample of text or speech'''\n",
        "  joined_docs = []\n",
        "\n",
        "  for line in documents:\n",
        "    joined_docs.append(' '.join(line))\n",
        "\n",
        "  return joined_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XVC8J4D9jCL"
      },
      "source": [
        "def remove_punctuation(documents):\n",
        "\n",
        "  punct_removed = []\n",
        "\n",
        "  for doc in documents:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      if word not in string.punctuation:\n",
        "        temp.append(word)\n",
        "    \n",
        "    punct_removed.append(temp)\n",
        "\n",
        "  return punct_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXJ3fd2bG8f2"
      },
      "source": [
        "def remove_stopwords(documents):\n",
        "  \n",
        "  stopword_removed = []\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  for doc in documents:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      if word not in stop_words:\n",
        "        temp.append(word)\n",
        "    \n",
        "    stopword_removed.append(temp)\n",
        "\n",
        "  return stopword_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iNIN5t8SVLR"
      },
      "source": [
        "def apply_stemmer(documents):\n",
        "  stemmed_docs = []\n",
        "  \n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  for doc in documents:\n",
        "    stemmed_docs.append([stemmer.stem(plural) for plural in doc])\n",
        "\n",
        "  return stemmed_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRF2oUJdwG-J"
      },
      "source": [
        "def identity(X):\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79khV6Vli2e3"
      },
      "source": [
        "def vec_tfidf(tfidf = True):\n",
        "\n",
        "  if tfidf:\n",
        "    vec = TfidfVectorizer(preprocessor = identity, analyzer='word',\n",
        "                          tokenizer = identity, ngram_range = (1,3))\n",
        "    # vec = TfidfVectorizer(preprocessor = identity, \n",
        "    #                       tokenizer = identity)\n",
        "  else:\n",
        "    # vec = CountVectorizer(preprocessor = identity, lowercase=True, analyzer='char',\n",
        "    #                       tokenizer = identity, ngram_range = (2,5))\n",
        "    \n",
        "    vec = CountVectorizer(preprocessor = identity,\n",
        "                          tokenizer = identity)\n",
        "    \n",
        "  return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eOqnRBvqBE7"
      },
      "source": [
        "def SVM_Static(train_docs, train_lbls, test_docs, test_lbls):\n",
        "\n",
        "  vec = vec_tfidf(tfidf = True)\n",
        "    \n",
        "  # combines the vectorizer with the SVM classifier\n",
        "  classifier = Pipeline([('vec', vec),\n",
        "                         ('cls', SVC(kernel='linear'))])\n",
        "  \n",
        "  classifier.fit(train_docs, train_lbls)\n",
        "\n",
        "    # predict is for predicting label for document test data by using predict method\n",
        "  prediction = classifier.predict(test_docs)\n",
        "\n",
        "  print(\"SVM Accuracy = \", accuracy_score(test_lbls, prediction))\n",
        "  print()\n",
        "\n",
        "  print(classification_report(test_lbls, prediction, labels=classifier.classes_, digits=3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71durTnXufyb"
      },
      "source": [
        "def Naive_Bayes(train_docs, train_lbls, test_docs, test_lbls):\n",
        "\n",
        "  vec = vec_tfidf(tfidf = False)\n",
        "    \n",
        "  # combines the vectorizer with the Naive Bayes classifier\n",
        "  classifier = Pipeline([('vec', vec),\n",
        "                         ('cls', MultinomialNB())])\n",
        "  \n",
        "  classifier.fit(train_docs, train_lbls)\n",
        "\n",
        "  prediction = classifier.predict(test_docs)\n",
        "\n",
        "  print(\"Naive Bayes Accuracy = \", accuracy_score(test_lbls, prediction))\n",
        "  print()\n",
        "\n",
        "  print(classification_report(test_lbls, prediction, labels=classifier.classes_, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUuNqFR72t8"
      },
      "source": [
        "def pre_processing(documents):\n",
        "\n",
        "  documents = remove_url(documents)\n",
        "\n",
        "  # documents = remove_hashtag(documents)\n",
        "\n",
        "  documents = remove_whitespaces(documents)\n",
        "\n",
        "  # documents = text_lowercasing(documents)\n",
        "\n",
        "  documents = tokenize_sentence(documents)\n",
        "\n",
        "  documents = remove_punctuation(documents)\n",
        "\n",
        "  documents = remove_stopwords(documents)\n",
        "\n",
        "  documents = apply_stemmer(documents)\n",
        "\n",
        "  # If we use character n_gram you have to enable it | else comment the below line\n",
        "  documents = char_n_gram_ready(documents)\n",
        "\n",
        "  return documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwTgh-aarnRQ",
        "outputId": "b2883dc6-d4b6-4679-d2b0-098ef3cda214"
      },
      "source": [
        "def main():\n",
        "  print('Reading The Dataset...')\n",
        "  \n",
        "  # Reading the training data\n",
        "  training_dataset = read_files('/content/drive/MyDrive/Colab Notebooks/datasets/corona_data/train.tsv')\n",
        "  train_labels, train_docs = separate_labels(training_dataset)\n",
        "\n",
        "  # Reading the test data\n",
        "  test_dataset = read_files('/content/drive/MyDrive/Colab Notebooks/datasets/corona_data/test.tsv')\n",
        "  test_labels, test_docs = separate_labels(test_dataset)\n",
        "  \n",
        "  # calling the pre processing dunction\n",
        "  train_docs = pre_processing(train_docs)\n",
        "  test_docs = pre_processing(test_docs)\n",
        "  # print(train_docs)\n",
        "\n",
        "  print('\\nTraining the Classifier...')\n",
        "  SVM_Static(train_docs, train_labels, test_docs, test_labels)\n",
        "  Naive_Bayes(train_docs, train_labels, test_docs, test_labels)\n",
        "\n",
        "  for lbl, doc in zip(train_labels[:5], train_docs[:5]):\n",
        "    print(lbl)\n",
        "    print(doc)\n",
        "    print()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading The Dataset...\n",
            "\n",
            "Training the Classifier...\n",
            "SVM Accuracy =  0.4\n",
            "\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative      0.479     0.240     0.320       146\n",
            "Extremely Positive      0.591     0.340     0.431       162\n",
            "          Negative      0.381     0.483     0.426       302\n",
            "           Neutral      0.453     0.414     0.433       152\n",
            "          Positive      0.324     0.424     0.367       238\n",
            "\n",
            "          accuracy                          0.400      1000\n",
            "         macro avg      0.446     0.380     0.396      1000\n",
            "      weighted avg      0.427     0.400     0.399      1000\n",
            "\n",
            "Naive Bayes Accuracy =  0.269\n",
            "\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative      0.296     0.199     0.238       146\n",
            "Extremely Positive      0.272     0.136     0.181       162\n",
            "          Negative      0.335     0.255     0.289       302\n",
            "           Neutral      0.246     0.474     0.324       152\n",
            "          Positive      0.232     0.290     0.257       238\n",
            "\n",
            "          accuracy                          0.269      1000\n",
            "         macro avg      0.276     0.271     0.258      1000\n",
            "      weighted avg      0.281     0.269     0.262      1000\n",
            "\n",
            "Neutral\n",
            "menyrbi phil_gahan chrisitv\n",
            "\n",
            "Positive\n",
            "advic talk neighbour famili exchang phone number creat contact list phone number neighbour school employ chemist GP set onlin shop account poss adequ suppli regular med order\n",
            "\n",
            "Positive\n",
            "coronaviru australia woolworth give elderli disabl dedic shop hour amid covid-19 outbreak\n",
            "\n",
            "Positive\n",
            "My food stock one empti ... pleas n't panic there will BE enough food for everyon take need stay calm stay safe covid19fr covid_19 covid19 coronaviru confin confinementot confinementgener\n",
            "\n",
            "Extremely Negative\n",
            "Me readi go supermarket covid19 outbreak not I 'm paranoid food stock litterali empti the coronaviru seriou thing pleas n't panic It caus shortag ... coronavirusfr restezchezv stayathom confin\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}